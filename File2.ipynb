{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ctrl408/Empirically-Derived-Scaling-Principles-for-an-increasingly-complex-binary-decision-task/blob/main/File2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4gjGf0XhpXy",
        "outputId": "20cf9a30-fa9d-43e6-b43b-f416152371ae"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [-0.8  0.8]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "Accuracy:  0.0\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [-0.9  0.9]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "Accuracy:  0.5\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [ 0.4 -0.4]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "Accuracy:  0.35\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [-0.7  0.7]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "Accuracy:  0.4\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [ 0.5 -0.5]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n",
            "Accuracy:  0.35\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "hidden layers: 1 n: 100 d: 2 nodes [2, 3, 1] coefficients [-0.95  0.95]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from numpy import vstack\n",
        "import itertools \n",
        "import torch \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Tanh\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "import sklearn \n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.model_selection import train_test_split \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Generate DataSet \n",
        "class DataSet:\n",
        "  \n",
        "  def data(n=100,d=5,W=None):\n",
        "   \n",
        "    global samples \n",
        "    samples=list()\n",
        "    n_1=0\n",
        "    n_0=0\n",
        "    n_samples=False\n",
        "\n",
        "    while n_samples==False:\n",
        "      A=np.random.uniform(low=0,high=1,size=d)\n",
        "      A_pow=list()\n",
        "      for i in range(1,(d+1)):\n",
        "        A_pow.append(A[(i-1)]**(i))   \n",
        "      pdt= np.dot(W.T,np.array(A_pow))\n",
        "    \n",
        "      if pdt >0:\n",
        "        if (n_1)==(n/2):pass\n",
        "        else:\n",
        "          samples.append([A,1])\n",
        "          n_1+=1\n",
        "      else:\n",
        "        if (n_0)==(n/2):pass\n",
        "        else:\n",
        "          samples.append([A,-1])\n",
        "          n_0+=1\n",
        "      if len(samples)==n:\n",
        "        n_samples=True\n",
        "    \n",
        "    X,Y=list(),list()  \n",
        "    for i in samples:      \n",
        "      X.append(i[0].tolist())\n",
        "      Y.append(i[1])\n",
        "    X=np.array(X)\n",
        "    Y=np.array(Y)\n",
        "   \n",
        "    X1,X2,Y1,Y2=train_test_split(X,Y, test_size=.20, random_state=42, shuffle=True, stratify=None)    \n",
        "    X1,Y1=torch.from_numpy(X1).type(torch.float32),torch.from_numpy(Y1).type(torch.float32)\n",
        "    dset=TensorDataset(X1,Y1) \n",
        "    trainloader=torch.utils.data.DataLoader(dset, batch_size=60, shuffle=False, num_workers=1)\n",
        "    X2,Y2=torch.from_numpy(X2).type(torch.float32),torch.from_numpy(Y2).type(torch.float32)      \n",
        "    dset2=TensorDataset(X2,Y2)           \n",
        "    testloader=torch.utils.data.DataLoader(dset2, batch_size=60, shuffle=False, num_workers=1)\n",
        "    \n",
        "    return trainloader,testloader \n",
        "\n",
        "\n",
        "class Net(Module):\n",
        "  \n",
        "  #MLP with adjustable width and depth \n",
        "  def MLP(width,depth): \n",
        "       \n",
        "      layers = []\n",
        "      width_l=list()\n",
        "      for i in range(1,width):width_l.append(i)\n",
        "      for wdth,dpth in zip(width_l,depth):\n",
        "        if wdth==1:\n",
        "          prev_wdth=1\n",
        "          prev_dpth=dpth\n",
        "          continue \n",
        "        layers.append(nn.Linear(prev_dpth,dpth))          \n",
        "        layers.append(nn.ReLU())\n",
        "        prev_dpth=dpth\n",
        "      layers.pop()\n",
        "      layers.append(Sigmoid())\n",
        "      global net\n",
        "      net = nn.Sequential(*layers)\n",
        "      print(net)\n",
        "      return net\n",
        "\n",
        "  def forward(x):\n",
        "    return net(x)\n",
        "\n",
        "class Main:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def Train(train_dl, model):\n",
        "    \n",
        "      Loss_Function= BCELoss()\n",
        "      optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "      #Training Loop\n",
        "      #total epochs=1000\n",
        "      for epoch in range(1000):\n",
        "          for i, (inputs, targets) in enumerate(train_dl):\n",
        "              inputs, targets=inputs.to(device), targets.to(device)\n",
        "              optimizer.zero_grad()            \n",
        "              yhat = model(inputs)                      \n",
        "              loss = Loss_Function(yhat, (targets.unsqueeze(1)))            \n",
        "              loss.backward()        \n",
        "              optimizer.step()\n",
        "  #Accuracy \n",
        "  def Test(test_dl, model):\n",
        "      predictions, actuals = list(), list()\n",
        "      for i, (inputs, targets) in enumerate(test_dl):\n",
        "          inputs, targets=inputs.to(device), targets.to(device)\n",
        "          yhat = model(inputs)        \n",
        "          yhat =(yhat.cpu()).detach().numpy()\n",
        "          actual = (targets.cpu()).numpy()\n",
        "          actual = actual.reshape((len(actual), 1))         \n",
        "          yhat = yhat.round()\n",
        "     \n",
        "          predictions.append(yhat)\n",
        "          actuals.append(actual)\n",
        "      predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "      \n",
        "      acc = accuracy_score(actuals, predictions)\n",
        "      print(\"Accuracy: \",acc)\n",
        "      return acc\n",
        "\n",
        "\n",
        "  def Predict(row, model):\n",
        "     \n",
        "      row = Tensor([row])     \n",
        "      yhat = model(row)\n",
        "      yhat = yhat.detach().numpy()\n",
        "      return yhat\n",
        "\n",
        "\n",
        "#Finds all possible combinations of hyperparameters within the specified range\n",
        "#orders the best combinations sequential order after training \n",
        "\n",
        "class Hyperparameters:\n",
        "\n",
        "   #Control Hyperparameters here\n",
        "   def __init__(self):\n",
        "\n",
        "     #Declare Range of Hyperparameters to Generate within \n",
        "     #eg: min layers=4 max layers=22 or min n=100 max n=1000\n",
        "     ####\n",
        "     #layers=[4,10]\n",
        "     n=[100,10000]\n",
        "     d=[2,10]\n",
        "     self.max_nodes=20\n",
        "     self.max_combinations_w=15\n",
        "     self.increment_w=.05\n",
        "     ####\n",
        "\n",
        "     #layers_cmb= np.arange(layers[0],layers[1],1)\n",
        "     n_cmb=np.arange(n[0],n[1],100)\n",
        "     d_cmb=np.arange(d[0],d[1],1)\n",
        "\n",
        "\n",
        "     #all possible combinations of hyperparameters \n",
        "     self.params=list(itertools.product(n_cmb,d_cmb))\n",
        "     self.param=[]\n",
        "     for i in self.params:\n",
        "       i=list(i)\n",
        "       i.insert(0,4)\n",
        "       self.param.append(i)\n",
        "\n",
        "\n",
        "   def initiate(self,*kwargs): \n",
        "\n",
        "     #Train model for every combinations of hyperparameters \n",
        "     #record parameters and accuracy, order [layers,n,d,nodes,accuracy]\n",
        "     self.data=list()   \n",
        "\n",
        "     for i in self.param:  \n",
        "             \n",
        "       #n combinations of nodes\n",
        "       if i[0]==2:\n",
        "         val=list()\n",
        "         val.insert(0,i[2])\n",
        "         val.append(1)\n",
        "         val=[val]\n",
        "         print(val)\n",
        "       else:\n",
        "         val=list()\n",
        "         r=np.arange((i[2]+1),(self.max_nodes),1)\n",
        "         A= sum([list(map(list, itertools.combinations(r, m))) for m in range(len(r) + 1)], [])  \n",
        "         val=list()   \n",
        "         for r in A:\n",
        "           if len(r)==(i[0]-3):\n",
        "             r.insert(0,i[2])\n",
        "             r.append(1)\n",
        "             val.append(r)\n",
        "\n",
        "       #n combinations of w\n",
        "       for j in val:\n",
        "         a,b=i[2]//2,i[2]%2         \n",
        "         c=np.ones(a)*-1\n",
        "         d=np.ones(a)\n",
        "         if b != 0:\n",
        "           c=np.append(c,-1)\n",
        "         sign=[]\n",
        "         for n in c.tolist():sign.append([n])\n",
        "         for n in d.tolist():sign.append([n])\n",
        "         param=(itertools.permutations(sign))\n",
        "         val=np.arange(0+(self.increment_w),(1+(self.increment_w)),self.increment_w)\n",
        "         val=np.append(val,(val)*-1)\n",
        "         n_w=False\n",
        "         w_cmb=[]\n",
        "         while n_w==False:\n",
        "           a=np.random.choice(val,i[2])                       \n",
        "         \n",
        "           if np.sum(a)==0:\n",
        "             if list(a) in w_cmb:               \n",
        "               continue \n",
        "             else:\n",
        "               w_cmb.append(list(a))\n",
        "\n",
        "           if len(w_cmb)==self.max_combinations_w:\n",
        "             n_w=True    \n",
        "         w_cmb=np.array(w_cmb)         \n",
        "         \n",
        "         #training loop\n",
        "         for k in w_cmb:\n",
        "           train_dl, test_dl = DataSet.data(n=i[1],d=i[2],W=k)\n",
        "           print(\"hidden layers:\",(i[0]-3),\"n:\",i[1],\"d:\",i[2],\"nodes\",j,\"coefficients\",k)           \n",
        "           model = (Net.MLP(i[0],j)).to(device)\n",
        "           Main.Train(train_dl, model)   \n",
        "           acc=Main.Test(test_dl, model) \n",
        "           #store layers,n,d,nodes,w,accuracy\n",
        "           self.data.append([(i[0]-1),i[1],i[2],j,k.tolist(),acc])  \n",
        "           print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "     return self.data,pd.DataFrame(self.data)\n",
        "\n",
        "   #Sort Hyperparameters According to accuracy \n",
        "   def best_param(self):\n",
        "     return self.data.sort(key=lambda x: x[4])\n",
        "\n",
        "\n",
        "\n",
        "hyp=Hyperparameters()\n",
        "data,pd_data=hyp.initiate()\n",
        "#save the data as csv file into drive\n",
        "#change path accordingly if needed\n",
        "pd_data.to_csv(\"drive/My Drive/Colab Notebooks/df.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkp3a3lQwF4pPi08fCraab",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}